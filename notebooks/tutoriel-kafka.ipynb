{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import des packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "from confluent_kafka import Producer, Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {'bootstrap.servers': 'xxxx:9092'}\n",
    "kadmin = AdminClient(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cr√©er un topic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_topic = [NewTopic('topicname', num_partitions=3, replication_factor=1),\n",
    "             NewTopic('test_kakfa', num_partitions=3, replication_factor=1)]\n",
    "fs = kadmin.create_topics(new_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic topicname created\n",
      "Topic test_kakfa created\n"
     ]
    }
   ],
   "source": [
    "for topic, f in fs.items():\n",
    "    try:\n",
    "        f.result()\n",
    "        print(\"Topic {} created\".format(topic))\n",
    "    except Exception as e:\n",
    "        print(\"Failed to create topic {}: {}\".format(topic, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lister les topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_confluent-controlcenter-6-2-0-1-cluster-rekey\n",
      "_confluent-controlcenter-6-2-0-1-KSTREAM-OUTEROTHER-0000000106-store-repartition\n",
      "_confluent-controlcenter-6-2-0-1-TriggerEventsStore-changelog\n",
      "_confluent-controlcenter-6-2-0-1-group-aggregate-store-ONE_MINUTE-changelog\n",
      "_confluent-metrics\n",
      "_confluent-controlcenter-6-2-0-1-group-aggregate-store-ONE_MINUTE-repartition\n",
      "_confluent-controlcenter-6-2-0-1-MonitoringStream-THREE_HOURS-repartition\n",
      "_confluent-controlcenter-6-2-0-1-monitoring-message-rekey-store\n",
      "_confluent-controlcenter-6-2-0-1-MonitoringStream-ONE_MINUTE-changelog\n",
      "_confluent-controlcenter-6-2-0-1-group-aggregate-store-THREE_HOURS-repartition\n",
      "_confluent-controlcenter-6-2-0-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition\n",
      "_confluent-controlcenter-6-2-0-1-KSTREAM-OUTEROTHER-0000000106-store-changelog\n",
      "_confluent-controlcenter-6-2-0-1-Group-ONE_MINUTE-changelog\n",
      "__transaction_state\n",
      "_confluent-controlcenter-6-2-0-1-Group-THREE_HOURS-changelog\n",
      "default_ksql_processing_log\n",
      "_confluent-controlcenter-6-2-0-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog\n",
      "docker-connect-status\n",
      "_confluent-controlcenter-6-2-0-1-group-stream-extension-rekey\n",
      "_confluent-controlcenter-6-2-0-1-Group-ONE_MINUTE-repartition\n",
      "_schemas\n",
      "_confluent-controlcenter-6-2-0-1-monitoring-aggregate-rekey-store-changelog\n",
      "_confluent-telemetry-metrics\n",
      "_confluent-controlcenter-6-2-0-1-Group-THREE_HOURS-repartition\n",
      "_confluent-controlcenter-6-2-0-1-MonitoringTriggerStore-changelog\n",
      "_confluent-controlcenter-6-2-0-1-TriggerActionsStore-repartition\n",
      "test_kakfa\n",
      "_confluent-controlcenter-6-2-0-1-MetricsAggregateStore-changelog\n",
      "_confluent-controlcenter-6-2-0-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog\n",
      "_confluent-ksql-default__command_topic\n",
      "_confluent-controlcenter-6-2-0-1-monitoring-aggregate-rekey-store-repartition\n",
      "_confluent-controlcenter-6-2-0-1-MonitoringStream-ONE_MINUTE-repartition\n",
      "_confluent-controlcenter-6-2-0-1-expected-group-consumption-rekey\n",
      "missionsidfmob\n",
      "_confluent-monitoring\n",
      "_confluent-controlcenter-6-2-0-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition\n",
      "_confluent-controlcenter-6-2-0-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog\n",
      "_confluent-controlcenter-6-2-0-1-MetricsAggregateStore-repartition\n",
      "_confluent-controlcenter-6-2-0-1-MonitoringVerifierStore-changelog\n",
      "_confluent-controlcenter-6-2-0-1-MonitoringTriggerStore-repartition\n",
      "_confluent-controlcenter-6-2-0-1-KSTREAM-OUTERTHIS-0000000105-store-changelog\n",
      "_confluent-controlcenter-6-2-0-1-monitoring-trigger-event-rekey\n",
      "__consumer_offsets\n",
      "_confluent_balancer_broker_samples\n",
      "_confluent-controlcenter-6-2-0-1-aggregate-topic-partition-store-repartition\n",
      "_confluent-controlcenter-6-2-0-1-TriggerEventsStore-repartition\n",
      "_confluent_balancer_partition_samples\n",
      "_confluent-license\n",
      "_confluent-controlcenter-6-2-0-1-AlertHistoryStore-changelog\n",
      "_confluent-controlcenter-6-2-0-1-MonitoringVerifierStore-repartition\n",
      "_confluent-controlcenter-6-2-0-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition\n",
      "docker-connect-offsets\n",
      "_confluent-controlcenter-6-2-0-1-TriggerActionsStore-changelog\n",
      "docker-connect-configs\n",
      "topicname\n",
      "_confluent-controlcenter-6-2-0-1-KSTREAM-OUTERTHIS-0000000105-store-repartition\n",
      "_confluent-controlcenter-6-2-0-1-MonitoringStream-THREE_HOURS-changelog\n",
      "_confluent-controlcenter-6-2-0-1-actual-group-consumption-rekey\n",
      "_confluent-controlcenter-6-2-0-1-AlertHistoryStore-repartition\n",
      "_confluent-controlcenter-6-2-0-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog\n",
      "_confluent_balancer_api_state\n",
      "_confluent-controlcenter-6-2-0-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition\n",
      "_confluent-controlcenter-6-2-0-1-metrics-trigger-measurement-rekey\n",
      "_confluent-command\n",
      "_confluent-controlcenter-6-2-0-1-group-aggregate-store-THREE_HOURS-changelog\n",
      "_confluent-controlcenter-6-2-0-1-aggregate-topic-partition-store-changelog\n",
      "app-missions-__assignor-__leader\n"
     ]
    }
   ],
   "source": [
    "topics = kadmin.list_topics().topics\n",
    "for item in topics.values():\n",
    "    print(item.topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supprimer un topic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_kakfa': <Future at 0x7f9189c2a810 state=running>}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kadmin.delete_topics([\"test_kakfa\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Producer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'topicname'\n",
    "producer = Producer(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producing record: alice\t{\"count\": 0}\n",
      "Producing record: alice\t{\"count\": 1}\n",
      "Producing record: alice\t{\"count\": 2}\n",
      "Producing record: alice\t{\"count\": 3}\n",
      "Producing record: alice\t{\"count\": 4}\n",
      "Producing record: alice\t{\"count\": 5}\n",
      "Producing record: alice\t{\"count\": 6}\n",
      "Producing record: alice\t{\"count\": 7}\n",
      "Producing record: alice\t{\"count\": 8}\n",
      "Producing record: alice\t{\"count\": 9}\n",
      "Produced record to topic topicname partition [2] @ offset 0\n",
      "Produced record to topic topicname partition [2] @ offset 1\n",
      "Produced record to topic topicname partition [2] @ offset 2\n",
      "Produced record to topic topicname partition [2] @ offset 3\n",
      "Produced record to topic topicname partition [2] @ offset 4\n",
      "Produced record to topic topicname partition [2] @ offset 5\n",
      "Produced record to topic topicname partition [2] @ offset 6\n",
      "Produced record to topic topicname partition [2] @ offset 7\n",
      "Produced record to topic topicname partition [2] @ offset 8\n",
      "Produced record to topic topicname partition [2] @ offset 9\n",
      "10 messages were produced to topic topicname!\n"
     ]
    }
   ],
   "source": [
    "delivered_records = 0\n",
    "\n",
    "def acked(err, msg):\n",
    "    global delivered_records\n",
    "    \"\"\"Delivery report handler called on\n",
    "    successful or failed delivery of message\n",
    "    \"\"\"\n",
    "    if err is not None:\n",
    "        print(\"Failed to deliver message: {}\".format(err))\n",
    "    else:\n",
    "        delivered_records += 1\n",
    "        print(\"Produced record to topic {} partition [{}] @ offset {}\"\n",
    "              .format(msg.topic(), msg.partition(), msg.offset()))\n",
    "\n",
    "for n in range(10):\n",
    "    record_key = \"alice\"\n",
    "    record_value = json.dumps({'count': n})\n",
    "    print(\"Producing record: {}\\t{}\".format(record_key, record_value))\n",
    "    producer.produce(topic, key=record_key, value=record_value, on_delivery=acked)\n",
    "    producer.poll(0)\n",
    "\n",
    "producer.flush()\n",
    "\n",
    "print(\"{} messages were produced to topic {}!\".format(delivered_records, topic))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consumer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_conf = conf\n",
    "consumer_conf['group.id'] = 'python_example_group_1'\n",
    "consumer_conf['auto.offset.reset'] = 'earliest'\n",
    "consumer = Consumer(consumer_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscribe to topic\n",
    "consumer.subscribe([topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consumed record with key b'alice' and value b'{\"count\": 0}',                   and updated total count to 0\n",
      "Consumed record with key b'alice' and value b'{\"count\": 1}',                   and updated total count to 1\n",
      "Consumed record with key b'alice' and value b'{\"count\": 2}',                   and updated total count to 3\n",
      "Consumed record with key b'alice' and value b'{\"count\": 3}',                   and updated total count to 6\n",
      "Consumed record with key b'alice' and value b'{\"count\": 4}',                   and updated total count to 10\n",
      "Consumed record with key b'alice' and value b'{\"count\": 5}',                   and updated total count to 15\n",
      "Consumed record with key b'alice' and value b'{\"count\": 6}',                   and updated total count to 21\n",
      "Consumed record with key b'alice' and value b'{\"count\": 7}',                   and updated total count to 28\n",
      "Consumed record with key b'alice' and value b'{\"count\": 8}',                   and updated total count to 36\n",
      "Consumed record with key b'alice' and value b'{\"count\": 9}',                   and updated total count to 45\n",
      "Waiting for message or event/error in poll()\n",
      "Waiting for message or event/error in poll()\n",
      "Waiting for message or event/error in poll()\n",
      "Waiting for message or event/error in poll()\n",
      "Waiting for message or event/error in poll()\n",
      "Waiting for message or event/error in poll()\n",
      "Waiting for message or event/error in poll()\n",
      "Waiting for message or event/error in poll()\n",
      "Waiting for message or event/error in poll()\n"
     ]
    }
   ],
   "source": [
    "# Process messages\n",
    "total_count = 0\n",
    "try:\n",
    "    while True:\n",
    "        msg = consumer.poll(1.0)\n",
    "        if msg is None:\n",
    "            print(\"Waiting for message or event/error in poll()\")\n",
    "            continue\n",
    "        elif msg.error():\n",
    "            print('error: {}'.format(msg.error()))\n",
    "        else:\n",
    "            # Check for Kafka message\n",
    "            record_key = msg.key()\n",
    "            record_value = msg.value()\n",
    "            data = json.loads(record_value)\n",
    "            count = data['count']\n",
    "            total_count += count\n",
    "            print(\"Consumed record with key {} and value {}, \\\n",
    "                  and updated total count to {}\"\n",
    "                  .format(record_key, record_value, total_count))\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topicname': <Future at 0x7f9189c17250 state=running>}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kadmin.delete_topics([\"topicname\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sources:**  \n",
    "[Github Confluent Kafka](https://github.com/confluentinc/confluent-kafka-python)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
